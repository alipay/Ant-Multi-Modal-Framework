# M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval

This repository contains codebase and data annotations for our paper [M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval](https://ieeexplore.ieee.org/document/10214396).

## What is M2-RAAP?

M2-RAAP is a multi-modal recipe for effective and efficient zero-shot video-text retrieval. Specifically, M2-RAAP 1) filters and refines video-text pairs to improve the data quality, 2) adopts key-frames as video inputs to reduce pre-training time, and 3) introduces temporal modeling and video feature enhancement to promote pre-training performance. Compared with the baselines, M2-RAAP employs only 10% of data volume (10M -> 1M) and consumes only 5% of pre-training time (1920h -> 92h), reaching a new SOTA on four English downstream zero-shot video-text retrieval datasets and two Chinese ones.


## Codebase

We are preparing codebase and data annotations, which will be available soon.

## Citation

If you find SNP-S3 useful, please consider citing the following paper:

```
@ARTICLE{10214396,
  author={Dong, Xingning and Guo, Qingpei and Gan, Tian and Wang, Qing and Wu, Jianlong and Ren, Xiangyuan and Cheng, Yuan and Chu, Wei},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TCSVT.2023.3303945}
}
```
