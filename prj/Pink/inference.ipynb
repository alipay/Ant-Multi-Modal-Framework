{"cells":[{"cell_type":"code","execution_count":null,"id":"90b018e0-401f-4b18-aac9-3f232e45fee6","metadata":{"execution":{"shell.execute_reply.end":"2023-09-21T07:30:18.423618Z","shell.execute_reply.started":"2023-09-21T07:30:12.855211Z","to_execute":"2023-09-21T07:30:12.831Z"},"trusted":true},"outputs":[],"source":["from transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria, CLIPModel\n","from torchvision.transforms.functional import InterpolationMode\n","from torchvision import transforms\n","import os\n","import sys\n","sys.path.append(\"./\")\n","from pink import *\n","import base64\n","import pandas as pd\n","import numpy as np\n","import torch\n","import json\n","\n","import io\n","from PIL import Image\n","import random\n","import math\n","from pink.datasets.Templates import ChoiceQuestionAnswer\n","from pink.conversation import conv_llava_v1, conv_simple_v1_mmbench, conv_llama2, conv_simple_v1\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"]},{"cell_type":"code","execution_count":null,"id":"b100cab3","metadata":{},"outputs":[],"source":["model_name = \"\"\n","config = AutoConfig.from_pretrained(model_name, use_cache=True)\n","config.llama_path = \"\"\n","if config.llama_path != model_name:\n","    # need to merge parameters\n","    llama_path = config.llama_path\n","    weight_map_index = json.load(open(os.path.join(llama_path, \"pytorch_model.bin.index.json\"), \"r\"))\n","    shard_files = list(set(weight_map_index[\"weight_map\"].values()))\n","    loaded_keys = weight_map_index[\"weight_map\"].keys()\n","    state_dict = {}\n","    for index, shard_file in enumerate(shard_files):\n","        state_dict.update(torch.load(os.path.join(llama_path, shard_file), map_location=\"cpu\"))\n","    peft_parameters = torch.load(os.path.join(model_name, \"saved_parameters.pth\"), map_location=\"cpu\")\n","    for k, v in peft_parameters.items():\n","        state_dict[k] = v\n","else:\n","    state_dict = None\n","\n","model = AutoModelForCausalLM.from_pretrained(None, config=config, state_dict=state_dict)\n","for name, param in model.model.named_parameters():\n","    if not (\"adapter_\" in name or \"lora_\" in name):\n","        param.data = param.data.half()\n","model.lm_head.to(torch.float16)\n","model = model.cuda()\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, padding_side='left')"]},{"cell_type":"code","execution_count":null,"id":"07ff19f2-670d-4ece-bf1c-090b73e71c30","metadata":{"execution":{"shell.execute_reply.end":"2023-09-19T05:31:57.890953Z","shell.execute_reply.started":"2023-09-19T05:31:57.866840Z","to_execute":"2023-09-19T05:31:57.582Z"},"trusted":true},"outputs":[],"source":["from pink.conversation import conv_llama2\n","image_processor = transforms.Compose(\n","    [\n","        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n","    ])\n","image_token_len = model.config.num_patches\n","\n","model.eval()\n","conv = conv_llama2.copy()\n","DEFAULT_IMAGE_TOKEN = \"<image>\"\n","DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n","DEFAULT_IM_START_TOKEN = \"<im_start>\"\n","DEFAULT_IM_END_TOKEN = \"<im_end>\"\n","PREFIX_IMAGE = \"Image: \"\n","PREFIX_NO_IMAGE = \"Image: N/A\"\n","BEGIN_DESCRIPTION = \"<des>\"\n","END_DESCRIPTION = \"</des>\"\n","BEGIN_LOC = \"<loc>\"\n","END_LOC = \"</loc>\"\n","BEGIN_CLS = \"<cls>\"\n","END_CLS = \"</cls>\"\n","BEGIN_RELATION = \"<rel>\"\n","END_RELATION = \"</rel>\"\n","BEGIN_QUESTION = \"<qes>\"\n","END_QUESTION = \"</qes>\"\n","DEFAULT_EOS_TOKEN = \"</s>\""]},{"cell_type":"code","execution_count":null,"id":"66311147-ac2d-4c65-a2d1-407882e282aa","metadata":{"execution":{"shell.execute_reply.end":"2023-09-21T03:46:05.518183Z","shell.execute_reply.started":"2023-09-21T03:46:05.514595Z","to_execute":"2023-09-21T03:46:05.401Z"},"trusted":true},"outputs":[],"source":["import re\n","from PIL import ImageDraw\n","from matplotlib import pyplot as plt\n","\n","image_path = \"./47.jpg\"\n","question = \"Locate Yaoming in the image?\"\n"]},{"cell_type":"code","execution_count":null,"id":"63473ce2-9f09-4e04-aa73-4ae76dcfb10f","metadata":{"execution":{"shell.execute_reply.end":"","shell.execute_reply.started":"2023-09-21T03:46:06.412980Z","to_execute":"2023-09-21T03:46:06.298Z"},"trusted":true},"outputs":[],"source":["loc_pattern = re.compile(r\"(\\[[0-9].[0-9][0-9][0-9],[0-9].[0-9][0-9][0-9],[0-9].[0-9][0-9][0-9],[0-9].[0-9][0-9][0-9]\\])\")\n","bbox_pattern = re.compile(r\"[0-9].[0-9][0-9][0-9]\")\n","\n","with open(image_path, \"rb\") as f:\n","    image = Image.open(io.BytesIO(f.read())).convert('RGB')\n","width, height = image.size\n","\n","copy_image1 = image.copy()\n","\n","image_tensor = image_processor(image)\n","images = image_tensor.unsqueeze(0).cuda()\n","conv.messages = []\n","conv.append_message(conv.roles[0], question)\n","conv.append_message(conv.roles[1], None)\n","conv.set_system(PREFIX_IMAGE + image_token_len * DEFAULT_IMAGE_PATCH_TOKEN)\n","cur_prompt = conv.get_prompt()\n","print(cur_prompt)\n","\n","tokenized_output = tokenizer(\n","    [cur_prompt],\n","    return_tensors=\"pt\",\n","    padding=\"longest\",\n","    max_length=tokenizer.model_max_length,\n","    truncation=True,\n",")\n","\n","input_ids = torch.as_tensor(tokenized_output.input_ids).cuda()\n","attention_mask = torch.as_tensor(tokenized_output.attention_mask).cuda()\n","\n","with torch.inference_mode():\n","    output_ids = model.generate(\n","        input_ids,\n","        images=images,\n","        has_images=[True],\n","        attention_mask=attention_mask,\n","        do_sample=False,\n","        num_beams=1,\n","        temperature=0.7,\n","        max_new_tokens=1024,\n","    )\n","\n","for input_id, output_id in zip(input_ids, output_ids):\n","    input_token_len = input_id.shape[0]\n","    n_diff_input_output = (input_id != output_id[:input_token_len]).sum().item()\n","    if n_diff_input_output > 0:\n","        print(f'[Warning] Sample {i}: {n_diff_input_output} output_ids are not the same as the input_ids')\n","    output = tokenizer.batch_decode(output_id[input_token_len:].unsqueeze(0), skip_special_tokens=True)[0]\n","    output = output.strip()\n","    print(output)\n","\n","loc_token = loc_pattern.findall(output)\n","\n","colors = [(0,0,255),(255,0,0),(0,255,0),(255,0,255),(255,255,0),(255,255,255),(0,0,0)]\n","for loc_id, loc in enumerate(loc_token):\n","    bbox = bbox_pattern.findall(loc)\n","    assert len(bbox) == 4\n","    scaled_bbox = [float(bbox[0])*width,float(bbox[1])*height,float(bbox[2])*width,float(bbox[3])*height]\n","    draw1 = ImageDraw.Draw(copy_image1)\n","    draw1.rectangle(scaled_bbox, fill=None, outline=colors[loc_id], width=5)\n","plt.imshow(copy_image1)\n"]},{"cell_type":"code","execution_count":null,"id":"ae086922","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
