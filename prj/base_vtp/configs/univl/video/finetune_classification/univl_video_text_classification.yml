task_attributes:
  univl_task:
    dataset_size_proportional_sampling: true
    dataset_attributes:
      video_text_classification:
        pretraining: True # indicate dataset is built for pretraining
        data_root_dir: ../tests/data/video

        annotations:
          train: msrvtt_train.jsonl
          val: msrvtt_train.jsonl
          test: msrvtt_test.jsonl

        use_videos: True
        train_ensemble_n_clips: 8
        test_ensembel_n_clips: 2
        num_frm: 1  # sample frames of each clip

        # support lmdb database for faster reading, convert video to lmdb:
        # prj/univl/scripts/lmdb_conversion.py
        videos:
          train: data/mp4
          val: data/mp4
          test: data/mp4

        processors: &processors
          train_frame_processor:
            type: custom_transforms
            params:
              mode: sequential
              transforms:
                - type: ImageLongsideScaleAndPad
                  params:
                    max_size: 448
                    random_scale: false
                    pad: false
                - type: GroupNormalize
                  params: # detr mean/std
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]

          test_frame_processor:
            type: custom_transforms
            params:
              mode: sequential
              transforms:
                - type: ImageLongsideScaleAndPad
                  params:
                    max_size: 448
                    random_scale: false
                    pad: false
                - type: GroupNormalize
                  params: # detr mean/std
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]

          caption_processor: &caption_processor
            type: masked_bert_tokenizer
            params:
              max_seq_length: 20 # max_length for each text field
              mask_probability: 0
              trim_start_token: false # not output start token id, because text info is appended to image info in later encoding stage
              tokenizer_config:
                type: bert-base-uncased
                params:
                  model_type: bert
                  do_lower_case: true
              preprocessor:
                type: simple_sentence
                params: {}

model_attributes:
  univl:
    training_head_type: video_text_classification
    training_stage: stage1+stage2
    encoder_lr_decay: 1.0
    num_labels: 3

    hidden_size: &model_size 768
    half_model_size: &half_model_size 384

    with_text_encoder: true
    text_encoder:
      type: PretrainedTransformerEncoder
      params:
        gradient_checkpointing: false
        pretrained: true # initialize with BERT pretrained weights
        model_type: bert
        bert_model_name: bert-base-uncased
        # bert-base-chinese related params
        num_hidden_layers: 12 # use three layer of bert
        vocab_size: 30522
        num_segments: 2
        hidden_size: 768

    with_image_encoder: true
    image_encoder:
      type: DetrImageEncoder
      params: # forward_pass num_clips次，disable resnet gradient_checkpointing
        gradient_checkpointing: [false, false, false, false]
        pretrained: true
        encoder_type: resnet50
        freeze: false
        pool_type: avg
        num_output_features: -1
        freeze_bn: true
        replace_stride_with_dilation: [False, False, False]
        with_position_embedding: false
        position_embedding:
          type: sine # ['sine', 'learned']
          params:
            num_pos_feats: *half_model_size # should be half of transformer hidden dim
            normalize: true
        output_channels: *model_size # should be transoformer hidden dim

    with_img_embeddings: true
    img_embeddings:
      type: ClipVisualEmbedding
      params:
        max_position_embeddings: 50
        num_pos_feats: 768

    with_cross_encoder: true
    cross_encoder:
      type: PretrainedTransformerEncoder
      params:
        gradient_checkpointing: false
        pretrained: true # initialize with BERT pretrained weights
        model_type: bert
        bert_model_name: bert-base-uncased
        # bert-base-chinese related params
        num_hidden_layers: 3 # use three layer of bert
        vocab_size: 30522
        num_segments: 2
        hidden_size: 768

    losses:
      - type: cross_entropy

    metrics:
      - accuracy

optimizer_attributes:
  type: AdamW
  params:
    lr: 5e-5
    betas: [0.9, 0.98]
    weight_decay: 1e-3

training_parameters:
  trainer: base_trainer
  clip_norm_mode: all
  clip_gradients: true
  max_grad_l2_norm: 5.0
  lr_scheduler: true
  lr_ratio: 0.1
  lr_steps: # support epoch
  - 2000
  use_warmup: false
  warmup_factor: 0.1
  warmup_iterations: 500
  max_epochs: 20
  patience: 1000000
  batch_size: 2
  num_workers: 0
  task_size_proportional_sampling: true
  monitored_metric: accuracy
  metric_minimize: false
  report_format: csv
  snapshot_interval: 2
  log_interval: 1
  max_ckpt_num: 10
  load_pretrained: True
  pretrained_mapping:
    module.model.model.module: module.model.module

