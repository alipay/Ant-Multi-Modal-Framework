# M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance


Official trsnsformers inference code of the paper ["M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance"](https://www.arxiv.org/abs/2502.18778).


## Introduction
We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni's language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain.

<p align="center">
    <img src="./data/m2-omni.png" width="800"/>
<p>


## Model Downloads

You can download the model from both Huggingface and ModelScope.

<div align="center">

| **Model**              |   **Input modality**    | **Oput modality** |                                                                            **Download**                                                                            |
| :--------------------- | :---------------------: | :---------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| M2-Omni | Image,text,video,audio | Image,text,audio  | [ğŸ¤– ModelScope](https://www.modelscope.cn/models/M2Cognition/M2-omni) |
</div>




## Evaluation Results
Detailed evaluation results are reported in our [technical report](https://www.arxiv.org/abs/2502.18778).


## Quick Start 

### Install
```
# create envï¼ˆpython 3.8ï¼‰
conda create -n m2-omni python=3.8
source activate m2-omni

# clone rep
cd /YourPath/
git clone https://github.com/alipay/Ant-Multi-Modal-Framework

# install
cd ./Ant-Multi-Modal-Framework/prj/M2_omni/
pip install -r requirements.txt
```

### ğŸ¤— Hugging Face Transformers

Here is a code snippet to show you how to use the chat model with `transformers`:

```python
import torch

from transformers import AutoProcessor, AutoTokenizer, GenerationConfig, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name_or_path = "your_model_name_or_path"

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map="cuda",
    _attn_implementation="flash_attention_2"
)

# default tokenizer and processer
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, add_bos_token=True)
processor = AutoProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)

assets_path = YOUR_ASSETS_PATH
```

```python
# QA
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚"}
        ],
    },
]
# Output:

# é¹¦é¹‰æ˜¯ä¸€ç±»éå¸¸å¤šæ ·åŒ–çš„é¸Ÿç±»ï¼Œåˆ†å¸ƒåœ¨å…¨çƒå„åœ°ï¼Œå°¤å…¶æ˜¯åœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºã€‚å®ƒä»¬ä»¥é²œè‰³çš„ç¾½æ¯›ã€èªæ˜çš„æ™ºåŠ›å’Œæ¨¡ä»¿äººç±»è¯­è¨€çš„èƒ½åŠ›è€Œé—»åã€‚ä»¥ä¸‹æ˜¯å¯¹é¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š
# ### 1. æ –æ¯åœ°\n- **çƒ­å¸¦é›¨æ—** è¿™æ˜¯é¹¦é¹‰æœ€å¸¸è§çš„æ –æ¯åœ°ï¼Œæä¾›äº†ä¸°å¯Œçš„é£Ÿç‰©èµ„æºå’Œé€‚å®œçš„ç”Ÿå­˜ç¯å¢ƒã€‚\n- **äºšçƒ­å¸¦æ£®æ—**ï¼šä¸€äº›ç§ç±»ä¹Ÿåˆ†å¸ƒåœ¨äºšçƒ­å¸¦åœ°åŒºï¼Œå¦‚éæ´²å’Œæ¾³å¤§åˆ©äºšã€‚\n- **å²›å±¿**ï¼šå¦‚åŠ æ‹‰å¸•æˆˆæ–¯ç¾¤å²›ä¸Šçš„é¹¦é¹‰ï¼Œç”Ÿæ´»åœ¨ç‹¬ç‰¹çš„å²›å±¿ç¯å¢ƒä¸­ã€‚
# ### 2. é¥®é£Ÿä¹ æƒ¯\n- **ä¸»è¦é£Ÿç‰©**ï¼šåŒ…æ‹¬æ°´æœã€åšæœã€ç§å­ã€èŠ±èœœã€æ˜†è™«å’Œå°å‹æ— è„Šæ¤åŠ¨ç‰©ã€‚\n- **ç‰¹æ®Šé¥®é£Ÿ**ï¼šæŸäº›ç§ç±»å¦‚é‡‘åˆšé¹¦é¹‰ï¼Œä¼šæ¶ˆè€—å¤§é‡çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ï¼Œè¿™äº›å…ƒç´ åœ¨å®ƒä»¬çš„åŸç”Ÿæ –æ¯åœ°ä¸­å¾ˆéš¾è·å¾—ã€‚
# ### 3. ç¤¾ä¼šè¡Œä¸º\n- **ç¾¤å±…æ€§**ï¼šå¤§å¤šæ•°é¹¦é¹‰æ˜¯ç¾¤å±…åŠ¨ç‰©ï¼Œé€šå¸¸ä»¥æˆå¯¹æˆ–å°ç¾¤ä½“å½¢å¼ç”Ÿæ´»ã€‚\n- **ç¤¾äº¤ç»“æ„**ï¼šæœ‰äº›ç§ç±»å¦‚éæ´²ç°é¹¦é¹‰ï¼ˆç°é¹¦é¹‰ï¼‰æœ‰å¤æ‚çš„ç¤¾ä¼šç­‰çº§ç³»ç»Ÿï¼Œè€Œå…¶ä»–ç§ç±»åˆ™è¾ƒä¸ºæ¾æ•£ã€‚
# ### 4. ç¹æ®–ä¹ æ€§\n- **ç¹æ®–å­£èŠ‚**ï¼šé€šå¸¸åœ¨ä¸€å¹´ä¸­çš„ç‰¹å®šå­£èŠ‚è¿›è¡Œç¹æ®–ï¼Œå…·ä½“æ—¶é—´å› ç§ç±»å’Œåœ°ç†ä½ç½®è€Œå¼‚ã€‚\n- **å·¢å€**ï¼šé€‰æ‹©æ ‘æ´ã€å²©çŸ³ç¼éš™æˆ–å…¶ä»–å®‰å…¨çš„éšè”½åœ°ç‚¹ç­‘å·¢ã€‚\n- **è›‹åµæ•°é‡**ï¼šä¸€èˆ¬äº§åµ2-6æšï¼Œå­µåŒ–æœŸçº¦ä¸º20-30å¤©ã€‚
# ### 5. é€ƒé¿å’Œé˜²å¾¡æœºåˆ¶\n- **é£è¡Œèƒ½åŠ›**ï¼šå°½ç®¡æœ‰äº›ç§ç±»å¦‚éæ´²ç°é¹¦é¹‰ä¸ä¼šé£ï¼Œä½†å®ƒä»¬ä¾ç„¶èƒ½å¤Ÿåˆ©ç”¨æ ‘æœ¨å’Œé«˜å¤„èº²é¿æ•é£Ÿè€…ã€‚\n- **å«å£°å’Œè­¦æŠ¥**

```

```python
# image qa
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": os.path.join(assets_path, "plant.png")},
            {"type": "text", "text": "What plant is this?"},
        ],
    },
]
# Output:

# The plant in this image is Dendrobium cruentum, a species of orchid. This orchid is known for its distinctive appearance, featuring long, slender leaves and delicate yellow flowers. Dendrobium cruentum is native to Southeast Asia and is prized by orchid enthusiasts for its unique beauty and relatively easy cultivation compared to some other orchid species. The plant's growth habit, with its upright stems and arching flower stalks, is characteristic of many Dendrobium orchids.
```

```python
# video qa
messages = [
    {
        "role": "user",
        "content": [
            {"type": "video", "video": os.path.join(assets_path, "video1.mp4")},
            {"type": "text", "text": "What is the woman doing?"},
        ],
    },
]
# Output:

# The woman is performing a series of yoga poses on a rooftop.

```

```python
# Video Audio QA
messages = [
    {
        "role": "user",
        "content": [
            {"type": "video", "video": os.path.join(assets_path, "video1.mp4")},
            {"type": "audio", "audio": os.path.join(assets_path, "audioqa.wav")},
        ],
    },
]
outputs = model.generate(messages, max_new_tokens=512)
# Output:

# The woman in the video is doing yoga.
```

```python
# multi-turn chat
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"},
        ],
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "åŒ—äº¬"},
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "å®ƒçš„å åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿæœ‰å¤šå°‘å¸¸ä½äººå£ï¼Ÿ"},
        ],
    },
]
# Output:

# åŒ—äº¬å¸‚çš„æ€»é¢ç§¯ä¸º16,410.54å¹³æ–¹åƒç±³ï¼Œå¸¸ä½äººå£çº¦ä¸º2157ä¸‡äººã€‚
```

```python
# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True, use_system=True
)

image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)

inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    audios=audio_inputs,
    return_tensors="pt",
)

inputs = inputs.to(device)
for k in inputs.keys():
    if k == "pixel_values" or k == "pixel_values_videos":
        inputs[k] = inputs[k].to(dtype=torch.bfloat16)

# setting generation configure
generation_config = {
    "top_p": 0.8,
    "top_k": 100,
    "temperature": 0.7,
    "do_sample": True,
    "repetition_penalty": 1.05
}
generation_config = GenerationConfig.from_dict(generation_config)

# Inference: Generation of the output
with torch.no_grad():
    generated_ids = model.generate(
        **inputs, max_new_tokens=512,
        eos_token_id=processor.gen_terminator,
        generation_config=generation_config,
    )

generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

## Citations
If you find M2-omni useful for your work, please consider citing:
```
@misc{guo2025m2omniadvancingomnimllmcomprehensive,
      title={M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance}, 
      author={Qingpei Guo and Kaiyou Song and Zipeng Feng and Ziping Ma and Qinglong Zhang and Sirui Gao and Xuzheng Yu and Yunxiao Sun and Tai-Wei Chang and Jingdong Chen and Ming Yang and Jun Zhou},
      year={2025},
      eprint={2502.18778},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.18778}, 
}
```

## License and Legal Disclaimer

This code repository is licensed under the [MIT License](../../LICENSE.txt), and the Legal Disclaimer is located in the [LEGAL.md file](../../LEGAL.md) under the project's root directory.
