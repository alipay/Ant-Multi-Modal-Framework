includes:
- ./base.yml
- ./video_swin.yml # use videoswin as visual encoder

task_attributes:
  univl_task:
    dataset_attributes:
      video_text_pretrain:
        data_root_dir: /YourPath/

        annotations:
          train:
            - annotations/coco_train2017_pretrain_format_with_subfix.jsonl
            - annotations/image_val_left.jsonl
            - annotations/region_desc_pretrain_format_with_subfix.jsonl
            - annotations/cc_train.jsonl
          val: annotations/image_val.jsonl
          test: annotations/image_val.jsonl

        train_ensemble_n_clips: 1 # image consider as 1 clip with num_frm=1
        test_ensembel_n_clips: 1
        num_frm: 1

        images:
          train: .
          val: .
          test: .

        processors:
          caption_processor: &caption_processor
            type: masked_bert_tokenizer
            params:
              max_seq_length: 30 # max_length for each text field
              mask_probability: 0.15


# coco image + visual genome: 15.1 k images, 5.6M training image-text pairs
training_parameters:
  lr_scheduler: true
  lr_ratio: 0.1
  lr_epochs: []
  lr_steps:
    - 110000
  max_epochs: 30
  use_warmup: true
  warmup_factor: 0.1
  warmup_iterations: 2000
  max_iterations: 130000
  batch_size: 128